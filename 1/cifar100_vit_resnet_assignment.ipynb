{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8f983b",
   "metadata": {},
   "source": [
    "\n",
    "# پروژه سوم بینایی کامپیوتر — ViT-Base-16 در برابر ResNet-50 (PyTorch, CIFAR-100)\n",
    "\n",
    "این نوت‌بوک طبق **بخش‌های تمرین** پیاده‌سازی شده است:\n",
    "1) **Setup & Data Preparation** (دانلود/پیش‌پردازش/افزودن اغتشاش/تقسیم‌بندی)\n",
    "2) **Model Preparation** (ViT-Base-16 از `timm` و ResNet-50 از `torchvision` + تعویض هد)\n",
    "3) **Training & Fine-tuning** (مرحله‌ی فریز هد + فاین‌تیون کامل، AdamW/SGD، EarlyStopping، ReduceLROnPlateau)\n",
    "4) **Evaluation Metrics** (Top-1 روی train/val/test + زمان‌ها + پارامترها + FLOPs)\n",
    "5) **Robustness Testing (Optional)** (نویز گاوسی و محو‌سازی مربعی)\n",
    "6) **Interpretability (ViT)** (Attention Rollout)\n",
    "7) **Plots & Report Artifacts** (نمودارها و فایل‌های متنی نتایج در `./outputs`)\n",
    "\n",
    "> **Dataset:** CIFAR-100  \n",
    "> **Image Size:** 224×224 (برای سازگاری با ViT و ResNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58198be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# اگر پکیج‌ها را ندارید، این را اجرا کنید (در Colab ممکن است لازم باشد).\n",
    "# !pip install timm thop matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06facc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import timm\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from thop import profile\n",
    "    THOP_AVAILABLE = True\n",
    "except Exception:\n",
    "    THOP_AVAILABLE = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self): self.sum, self.cnt = 0.0, 0\n",
    "    def update(self, v, n=1): self.sum += v*n; self.cnt += n\n",
    "    @property\n",
    "    def avg(self): return self.sum / max(1, self.cnt)\n",
    "\n",
    "def accuracy(outputs, targets):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b4d6c",
   "metadata": {},
   "source": [
    "## 1) Setup & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f52fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CIFAR_MEAN = (0.5071, 0.4865, 0.4409)\n",
    "CIFAR_STD  = (0.2673, 0.2564, 0.2762)\n",
    "\n",
    "def get_transforms(img_size=224, train=True):\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(img_size, padding=4),\n",
    "            transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ])\n",
    "\n",
    "def get_dataloaders(data_dir=\"./data\", img_size=224, batch_size=128, num_workers=4, val_split=0.1, seed=42):\n",
    "    train_set = datasets.CIFAR100(root=data_dir, train=True, download=True, transform=get_transforms(img_size, True))\n",
    "    test_set  = datasets.CIFAR100(root=data_dir, train=False, download=True, transform=get_transforms(img_size, False))\n",
    "    n_train = len(train_set)\n",
    "    n_val = int(n_train * val_split)\n",
    "    n_train = n_train - n_val\n",
    "    gen = torch.Generator().manual_seed(seed)\n",
    "    train_subset, val_subset = torch.utils.data.random_split(train_set, [n_train, n_val], generator=gen)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb10507",
   "metadata": {},
   "source": [
    "## 2) Model Preparation (ViT-Base-16 و ResNet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(model_name: str, num_classes=100, pretrained=True):\n",
    "    if model_name == \"vit\":\n",
    "        model = timm.create_model(\"vit_base_patch16_224\", pretrained=pretrained, num_classes=num_classes)\n",
    "        assert isinstance(model, VisionTransformer)\n",
    "        return model\n",
    "    elif model_name == \"resnet\":\n",
    "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
    "        in_features = m.fc.in_features\n",
    "        m.fc = nn.Linear(in_features, num_classes)\n",
    "        return m\n",
    "    else:\n",
    "        raise ValueError(\"model_name must be 'vit' or 'resnet'\")\n",
    "\n",
    "def freeze_backbone(model, model_name):\n",
    "    if model_name == \"vit\":\n",
    "        for p in model.parameters(): p.requires_grad = False\n",
    "        for p in model.head.parameters(): p.requires_grad = True\n",
    "    else:\n",
    "        for name, p in model.named_parameters(): p.requires_grad = False\n",
    "        for p in model.fc.parameters(): p.requires_grad = True\n",
    "\n",
    "def unfreeze_all(model):\n",
    "    for p in model.parameters(): p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7431d1",
   "metadata": {},
   "source": [
    "## 3) Training & Fine-tuning (+ EarlyStopping, Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=True):\n",
    "        self.patience = patience; self.counter = 0; self.best = -float('inf'); self.stop=False; self.verbose=verbose\n",
    "    def step(self, metric):\n",
    "        if metric > self.best + 1e-8:\n",
    "            self.best = metric; self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose: print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience: self.stop=True\n",
    "\n",
    "def epoch_pass(model, loader, optimizer=None, scaler=None):\n",
    "    train = optimizer is not None\n",
    "    model.train(train)\n",
    "    lm, am = AverageMeter(), AverageMeter()\n",
    "    t0 = time.time()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "                out = model(x); loss = F.cross_entropy(out, y)\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                loss.backward(); optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = model(x); loss = F.cross_entropy(out, y)\n",
    "        am.update(accuracy(out, y), x.size(0)); lm.update(loss.item(), x.size(0))\n",
    "    return lm.avg, am.avg, time.time()-t0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    lm, am = AverageMeter(), AverageMeter()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x); loss = F.cross_entropy(out, y)\n",
    "        am.update(accuracy(out, y), x.size(0)); lm.update(loss.item(), x.size(0))\n",
    "    return lm.avg, am.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee2f1a",
   "metadata": {},
   "source": [
    "## 4) Plots & Report Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86558c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_curves(hist, out_dir=\"./outputs\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.figure()\n",
    "    plt.plot(hist['train_acc'], label='train_acc'); plt.plot(hist['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend(); plt.savefig(os.path.join(out_dir,'acc_curve.png'), dpi=150); plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist['train_loss'], label='train_loss'); plt.plot(hist['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend(); plt.savefig(os.path.join(out_dir,'loss_curve.png'), dpi=150); plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cdf7b",
   "metadata": {},
   "source": [
    "## 5) Robustness Testing (اختیاری)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836dd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def eval_with_noise(model, loader, noise_std=0.1):\n",
    "    model.eval(); am = AverageMeter()\n",
    "    for x, y in loader:\n",
    "        x = x + noise_std * torch.randn_like(x)\n",
    "        x = torch.clamp(x, -5, 5)\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x); am.update(accuracy(out, y), x.size(0))\n",
    "    return am.avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_with_occlusion(model, loader, erase_size=32):\n",
    "    model.eval(); am = AverageMeter()\n",
    "    for x, y in loader:\n",
    "        b,c,h,w = x.shape\n",
    "        top = torch.randint(0, h-erase_size+1, (b,))\n",
    "        left= torch.randint(0, w-erase_size+1, (b,))\n",
    "        for i in range(b):\n",
    "            x[i, :, top[i]:top[i]+erase_size, left[i]:left[i]+erase_size] = 0.0\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x); am.update(accuracy(out, y), x.size(0))\n",
    "    return am.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6a843",
   "metadata": {},
   "source": [
    "## 6) Interpretability (ViT Attention Rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VitAttentionHook:\n",
    "    def __init__(self, model: VisionTransformer):\n",
    "        self.handles = []; self.attn_maps=[]\n",
    "        for blk in model.blocks:\n",
    "            h = blk.attn.attn_drop.register_forward_hook(self._hook); self.handles.append(h)\n",
    "    def _hook(self, module, inp, out):\n",
    "        self.attn_maps.append(inp[0].detach().cpu())\n",
    "    def remove(self):\n",
    "        for h in self.handles: h.remove()\n",
    "        self.handles=[]\n",
    "\n",
    "def attention_rollout(attn_list, discard_ratio=0.0):\n",
    "    result=None\n",
    "    for attn in attn_list:\n",
    "        attn = attn.mean(dim=1)  # [B,N,N]\n",
    "        I = torch.eye(attn.size(-1)).unsqueeze(0).expand_as(attn)\n",
    "        attn = attn + I\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        result = attn if result is None else torch.bmm(result, attn)\n",
    "    return result\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_vit_attention(model, img_tensor, out_path=\"vit_attention.png\", discard_ratio=0.0):\n",
    "    model.eval()\n",
    "    if not isinstance(model, VisionTransformer):\n",
    "        print(\"Visualization only for ViT.\"); return\n",
    "    hook = VitAttentionHook(model)\n",
    "    _ = model(img_tensor.to(DEVICE))\n",
    "    attn = attention_rollout(hook.attn_maps, discard_ratio)\n",
    "    hook.remove()\n",
    "    cls = attn[:,0,1:]\n",
    "    B = cls.size(0); num = cls.size(1); g = int(math.sqrt(num))\n",
    "    maps = cls.view(B,1,g,g)\n",
    "    maps = torch.nn.functional.interpolate(maps, size=(img_tensor.size(-2), img_tensor.size(-1)), mode='bilinear', align_corners=False)\n",
    "    m = maps[0,0].cpu().numpy()\n",
    "    plt.imshow(m, cmap='jet', alpha=0.7); plt.axis('off'); plt.title('ViT Attention Rollout')\n",
    "    os.makedirs(\"./outputs\", exist_ok=True)\n",
    "    plt.savefig(\"./outputs/vit_attention.png\", dpi=150, bbox_inches='tight'); plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99ecb9",
   "metadata": {},
   "source": [
    "## 7) Evaluation Metrics (FLOPs/Params/Timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_flops(model, img_size=224):\n",
    "    if not THOP_AVAILABLE: return None\n",
    "    dummy = torch.randn(1,3,img_size,img_size).to(DEVICE)\n",
    "    macs, params = profile(model, (dummy,), verbose=False)\n",
    "    return int(macs*2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_inference_time(model, img_size=224, repeats=50):\n",
    "    model.eval()\n",
    "    dummy = torch.randn(1,3,img_size,img_size).to(DEVICE)\n",
    "    for _ in range(10): _ = model(dummy)\n",
    "    if DEVICE.type=='cuda': torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(repeats): _ = model(dummy)\n",
    "    if DEVICE.type=='cuda': torch.cuda.synchronize()\n",
    "    return (time.time()-t0)/repeats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b204612",
   "metadata": {},
   "source": [
    "## 8) اجرای کامل (پیکربندی، آموزش، فاین‌تیون، ارزیابی، نمودارها)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== پیکربندی =====\n",
    "cfg = {\n",
    "    \"model\": \"vit\",        # \"vit\" یا \"resnet\"\n",
    "    \"data_dir\": \"./data\",\n",
    "    \"out_dir\": \"./outputs\",\n",
    "    \"img_size\": 224,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 30,\n",
    "    \"ft_epochs\": 15,\n",
    "    \"freeze_stage\": True,  # مرحله 1: آموزش فقط هد\n",
    "    \"finetune_stage\": True,# مرحله 2: فاین‌تیون کامل\n",
    "    \"lr\": None,            # اگر None باشد، مقادیر مناسب پیش‌فرض انتخاب می‌شود\n",
    "    \"ft_lr\": None,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"num_workers\": 4,\n",
    "    \"val_split\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    \"es_patience\": 5,\n",
    "    \"amp\": True,\n",
    "    \"resnet_adamw\": False\n",
    "}\n",
    "\n",
    "set_seed(cfg[\"seed\"])\n",
    "os.makedirs(cfg[\"out_dir\"], exist_ok=True)\n",
    "\n",
    "# ===== داده =====\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_dir=cfg[\"data_dir\"],\n",
    "    img_size=cfg[\"img_size\"],\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    num_workers=cfg[\"num_workers\"],\n",
    "    val_split=cfg[\"val_split\"],\n",
    "    seed=cfg[\"seed\"]\n",
    ")\n",
    "\n",
    "# ===== مدل =====\n",
    "model = build_model(cfg[\"model\"], num_classes=100, pretrained=True).to(DEVICE)\n",
    "if cfg[\"freeze_stage\"]:\n",
    "    freeze_backbone(model, cfg[\"model\"])\n",
    "\n",
    "total_params, trainable_params = count_params(model)\n",
    "print(f\"Total params: {total_params/1e6:.2f}M | Trainable: {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "# ===== بهینه‌ساز و شِدولر =====\n",
    "if cfg[\"model\"] == \"vit\":\n",
    "    lr = cfg[\"lr\"] if cfg[\"lr\"] is not None else (1e-3 if cfg[\"freeze_stage\"] else 5e-5)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=cfg[\"weight_decay\"])\n",
    "else:\n",
    "    lr = cfg[\"lr\"] if cfg[\"lr\"] is not None else 1e-3\n",
    "    if cfg[\"resnet_adamw\"]:\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=cfg[\"weight_decay\"])\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9, weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "early = EarlyStopping(patience=cfg[\"es_patience\"], verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=cfg[\"amp\"])\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val, best_path = -1.0, os.path.join(cfg[\"out_dir\"], f\"best_{cfg['model']}.pt\")\n",
    "\n",
    "print(\"=== Stage 1: Head-only training ===\" if cfg[\"freeze_stage\"] else \"=== Single-stage training ===\")\n",
    "for epoch in range(1, cfg[\"epochs\"]+1):\n",
    "    tr_loss, tr_acc, tr_time = epoch_pass(model, train_loader, optimizer, scaler)\n",
    "    va_loss, va_acc = evaluate(model, val_loader)\n",
    "    scheduler.step(va_acc)\n",
    "\n",
    "    history['train_loss'].append(tr_loss); history['train_acc'].append(tr_acc)\n",
    "    history['val_loss'].append(va_loss);   history['val_acc'].append(va_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f} | {tr_time:.1f}s\")\n",
    "\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"  >> Saved best: {best_path} (val_acc={best_val:.4f})\")\n",
    "\n",
    "    early.step(va_acc)\n",
    "    if early.stop:\n",
    "        print(\"Early stopping triggered.\"); break\n",
    "\n",
    "# ===== مرحلهٔ 2: فاین‌تیون کامل =====\n",
    "if cfg[\"finetune_stage\"]:\n",
    "    print(\"=== Stage 2: Full fine-tuning ===\")\n",
    "    if os.path.exists(best_path):\n",
    "        model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    unfreeze_all(model)\n",
    "\n",
    "    if cfg[\"model\"] == \"vit\":\n",
    "        ft_lr = cfg[\"ft_lr\"] if cfg[\"ft_lr\"] is not None else 5e-5\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=ft_lr, weight_decay=cfg[\"weight_decay\"])\n",
    "    else:\n",
    "        if cfg[\"resnet_adamw\"]:\n",
    "            ft_lr = cfg[\"ft_lr\"] if cfg[\"ft_lr\"] is not None else 1e-4\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=ft_lr, weight_decay=cfg[\"weight_decay\"])\n",
    "        else:\n",
    "            ft_lr = cfg[\"ft_lr\"] if cfg[\"ft_lr\"] is not None else 1e-3\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=ft_lr, momentum=0.9, weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "    early = EarlyStopping(patience=cfg[\"es_patience\"], verbose=True)\n",
    "\n",
    "    for epoch in range(1, cfg[\"ft_epochs\"]+1):\n",
    "        tr_loss, tr_acc, tr_time = epoch_pass(model, train_loader, optimizer, scaler)\n",
    "        va_loss, va_acc = evaluate(model, val_loader)\n",
    "        scheduler.step(va_acc)\n",
    "\n",
    "        history['train_loss'].append(tr_loss); history['train_acc'].append(tr_acc)\n",
    "        history['val_loss'].append(va_loss);   history['val_acc'].append(va_acc)\n",
    "\n",
    "        print(f\"[FT] Epoch {epoch:03d} | train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f} | {tr_time:.1f}s\")\n",
    "\n",
    "        if va_acc > best_val:\n",
    "            best_val = va_acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  >> Saved best: {best_path} (val_acc={best_val:.4f})\")\n",
    "\n",
    "        early.step(va_acc)\n",
    "        if early.stop:\n",
    "            print(\"Early stopping (fine-tune) triggered.\"); break\n",
    "\n",
    "# ===== ارزیابی تست =====\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"TEST: loss={test_loss:.4f} acc={test_acc:.4f}\")\n",
    "\n",
    "# ===== نمودارها =====\n",
    "plot_curves(history, cfg[\"out_dir\"])\n",
    "\n",
    "# ===== FLOPs و زمان =====\n",
    "flops = compute_flops(model, cfg[\"img_size\"])\n",
    "tot_p, tr_p = count_params(model)\n",
    "inf_t = measure_inference_time(model, cfg[\"img_size\"], repeats=50)\n",
    "\n",
    "with open(os.path.join(cfg[\"out_dir\"], f\"{cfg['model']}_metrics.txt\"), \"w\") as f:\n",
    "    f.write(f\"Test loss: {test_loss:.4f}\\nTest acc: {test_acc:.4f}\\n\")\n",
    "    f.write(f\"Total params: {tot_p}\\nTrainable params: {tr_p}\\n\")\n",
    "    f.write(f\"Inference time per sample (s): {inf_t}\\n\")\n",
    "    f.write(f\"FLOPs (approx): {flops if flops is not None else 'thop not installed'}\\n\")\n",
    "\n",
    "# ===== Robustness =====\n",
    "noise_acc = eval_with_noise(model, test_loader, noise_std=0.1)\n",
    "occ_acc   = eval_with_occlusion(model, test_loader, erase_size=32)\n",
    "with open(os.path.join(cfg[\"out_dir\"], f\"{cfg['model']}_robustness.txt\"), \"w\") as f:\n",
    "    f.write(f\"Gaussian noise std=0.1 acc: {noise_acc}\\n\")\n",
    "    f.write(f\"Occlusion 32x32 acc: {occ_acc}\\n\")\n",
    "\n",
    "# ===== تفسیرپذیری ViT =====\n",
    "if cfg[\"model\"] == \"vit\":\n",
    "    imgs, _ = next(iter(test_loader))\n",
    "    img0 = imgs[0:1].to(DEVICE)\n",
    "    visualize_vit_attention(model, img0, out_path=os.path.join(cfg[\"out_dir\"], \"vit_attention.png\"))\n",
    "\n",
    "print(\"Done. Outputs in:\", cfg[\"out_dir\"])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
