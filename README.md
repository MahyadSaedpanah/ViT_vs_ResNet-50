# ViT vs. ResNet for CIFAR-100 Classification

A modular PyTorch project comparing the performance of **Vision Transformer (ViT-Base-16)** and **ResNet-50** on the CIFAR-100 dataset.

---

## ğŸ“¦ Structure
```
vision_project/
â”œâ”€â”€ data/                # Data loading and augmentation
â”œâ”€â”€ models/              # ViT and ResNet definitions
â”œâ”€â”€ train/               # Training scripts
â”œâ”€â”€ evaluate/            # Accuracy, speed, FLOPs
â”œâ”€â”€ visualize/           # Attention & Grad-CAM
â”œâ”€â”€ utils/               # Helpers (seed, save/load, etc.)
â”œâ”€â”€ configs/             # config.yaml with hyperparameters
â”œâ”€â”€ main.py              # Main pipeline script
â””â”€â”€ README.md            # You are here
```

---

## ğŸ“Š Features
- âœ… Fine-tuning pretrained **ViT-Base-16** from `timm`
- âœ… Fine-tuning **ResNet-50** from `torchvision`
- âœ… Configurable training via YAML
- âœ… Modular & readable codebase
- âœ… Evaluation: accuracy, speed, model size, FLOPs
- âœ… Visualization: attention maps (ViT), Grad-CAM (ResNet)

---

## ğŸ§ª Quick Start
```bash
# Clone and install dependencies
pip install -r requirements.txt

# Run the full pipeline
python main.py
```

---

## âš™ï¸ Configuration
Edit `configs/config.yaml` to change:
- learning rates
- optimizers
- batch size / epochs
- freezing ViT layers

---

## ğŸ“ Dataset
Using CIFAR-100 (downloaded automatically).

---

## ğŸ“ˆ Evaluation Metrics
- Top-1 Accuracy (train / val / test)
- Training time per epoch
- Inference time per sample
- Number of parameters
- FLOPs (via `ptflops`)

---

## ğŸ“Œ Visualization
- `vit_attention.py`: attention map overlay for ViT
- `resnet_gradcam.py`: Grad-CAM for CNN explanation

---
